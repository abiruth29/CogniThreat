# Part 4: Dense Layers, Bayesian Reasoning & Final Prediction ðŸŽ¯

## SECTION 10: DENSE LAYERS - FROM QUANTUM TO CLASSICAL ðŸ”Œ

### 10.1 The Transition Point

**Where we are:**
```python
Quantum LSTM output: h_9 = (64,) hidden units
Values: [-0.963, -0.978, -0.945, -0.821, 0.092, ...]
        â†‘ Strong negative = attack patterns detected

Now: Convert to final binary classification (Attack vs Benign)
```

**Classical Dense Layers:**
```python
h_9 (64) â†’ Dense1 (128) â†’ ReLU â†’ Dropout â†’ Dense2 (64) â†’ ReLU â†’ Dense3 (2) â†’ Softmax
           â†‘ Expand      â†‘ Activate  â†‘ Regularize  â†‘ Compress  â†‘ Activate  â†‘ Classify
```

### 10.2 Dense Layer 1: Expansion (64 â†’ 128)

**Purpose:** Expand feature space to capture complex patterns

**Formula:**
```python
z_1 = W_1 Â· h_9 + b_1
a_1 = ReLU(z_1)

Where:
  W_1: (128, 64) weight matrix (learned)
  b_1: (128,) bias vector (learned)
  ReLU(x) = max(0, x)
```

**Step-by-Step Computation:**

```python
# Our input (final LSTM hidden state):
h_9 = np.array([
    0.092,   # unit 0: duration (normal)
    -0.963,  # unit 1: count (ATTACK!)
    -0.978,  # unit 2: serror (ATTACK!)
    -0.945,  # unit 3: hosts (ATTACK!)
    -0.821,  # unit 4: src_bytes (suspicious)
    0.034,   # unit 5: dst_bytes (normal)
    -0.956,  # unit 6: SYN flag (ATTACK!)
    0.018,   # unit 7: ACK flag (normal)
    # ... 56 more units
])

# Learned weights (showing first 4 neurons of 128):
W_1 = np.array([
    # Neuron 0: Focuses on "normal" patterns
    [0.23, -0.15, -0.18, -0.14, -0.12, 0.34, -0.16, 0.28, ...],
    
    # Neuron 1: Attack detector (high connection count)
    [-0.05, 0.89, 0.12, 0.15, 0.08, -0.02, 0.11, -0.04, ...],
    
    # Neuron 2: Error rate detector
    [-0.03, 0.15, 0.92, 0.18, 0.10, -0.01, 0.16, -0.03, ...],
    
    # Neuron 3: Multi-host scan detector
    [-0.04, 0.12, 0.14, 0.88, 0.09, -0.02, 0.13, -0.05, ...],
    
    # ... 124 more neurons
])

b_1 = np.array([0.1, -0.2, -0.15, -0.18, ...])  # 128 biases

# Compute first neuron output:
z_1[0] = sum(W_1[0] * h_9) + b_1[0]
       = 0.23*0.092 + (-0.15)*(-0.963) + (-0.18)*(-0.978) + ... + 0.1
       = 0.021 + 0.144 + 0.176 + ... + 0.1
       = 0.845

a_1[0] = ReLU(0.845) = max(0, 0.845) = 0.845

# Compute second neuron (attack detector):
z_1[1] = sum(W_1[1] * h_9) + b_1[1]
       = (-0.05)*0.092 + 0.89*(-0.963) + 0.12*(-0.978) + ... - 0.2
       = -0.005 - 0.857 - 0.117 + ... - 0.2
       = -1.567  â† Strong attack signal!

a_1[1] = ReLU(-1.567) = max(0, -1.567) = 0.0  â† DEAD NEURON!
```

**Wait, what happened? ðŸ¤”**

Neuron 1 detected a strong attack (-1.567) but ReLU killed it (â†’ 0)!

**This is actually GOOD design:**
- ReLU forces the network to learn POSITIVE activations for attacks
- Negative LSTM outputs get inverted by learned negative weights
- Let's see neuron 2 (error detector):

```python
# Neuron 2: Error rate detector
z_1[2] = sum(W_1[2] * h_9) + b_1[2]
       = (-0.03)*0.092 + 0.15*(-0.963) + 0.92*(-0.978) + ... - 0.15
       = -0.003 - 0.144 - 0.900 + ... - 0.15
       = -1.245

a_1[2] = ReLU(-1.245) = 0.0

# Hmm, also dead. Let's check a properly tuned neuron:

# Neuron 42: Trained to detect attack patterns (inverted weights)
W_1[42] = [0.12, -0.78, -0.85, -0.81, -0.67, 0.05, -0.79, 0.08, ...]
#                â†‘      â†‘      â†‘      â†‘             â†‘
#              NEGATIVE weights for NEGATIVE attack signals
#              â†’ POSITIVE output!

z_1[42] = 0.12*0.092 + (-0.78)*(-0.963) + (-0.85)*(-0.978) + ... + 0.2
        = 0.011 + 0.751 + 0.831 + 0.765 + ... + 0.2
        = 3.456  â† POSITIVE! ðŸŽ¯

a_1[42] = ReLU(3.456) = 3.456  â† ACTIVE ATTACK DETECTOR! ðŸš¨
```

**Key Insight:**
```python
Negative LSTM outputs + Negative learned weights = Positive activations!

h_9[2] = -0.978  (attack signal)
    Ã— 
W_1[42,2] = -0.85  (learned to invert)
    = 
+0.831  (positive activation survives ReLU!)
```

**Full Dense1 output:**
```python
# After processing all 64 LSTM outputs through 128 neurons:
a_1 = np.array([
    0.845,   # Neuron 0 (normal pattern detector)
    0.0,     # Neuron 1 (dead - poor weights)
    0.0,     # Neuron 2 (dead - poor weights)
    # ...
    3.456,   # Neuron 42 (ATTACK DETECTOR!) ðŸš¨
    2.983,   # Neuron 56 (attack pattern 2) ðŸš¨
    0.234,   # Neuron 78 (weak normal signal)
    3.821,   # Neuron 91 (STRONG ATTACK!) ðŸš¨ðŸš¨
    # ...
])

print(f"Dense1 output shape: {a_1.shape}")  # (128,)
print(f"Active neurons: {np.sum(a_1 > 0)}")  # 67 neurons active
print(f"Max activation: {np.max(a_1)}")      # 4.123 (strong attack!)
print(f"Mean activation: {np.mean(a_1)}")    # 0.834
```

### 10.3 Dropout Layer: Regularization

**Purpose:** Prevent overfitting by randomly dropping neurons during training

**Formula (Training):**
```python
During training (dropout_rate = 0.3):
  For each neuron i:
    With probability 0.3: a_1[i] = 0
    With probability 0.7: a_1[i] = a_1[i] / 0.7  (scale up)
```

**Formula (Inference/Testing):**
```python
During inference: NO DROPOUT!
  a_1_dropout = a_1  (use all neurons)
```

**Our case (inference mode):**
```python
# We're classifying a real sample, so NO dropout:
a_1_dropout = a_1  # All 128 neurons active

print(f"Dropout output: {a_1_dropout.shape}")  # (128,)
```

### 10.4 Dense Layer 2: Compression (128 â†’ 64)

**Purpose:** Compress to focused attack features

**Formula:**
```python
z_2 = W_2 Â· a_1_dropout + b_2
a_2 = ReLU(z_2)

Where:
  W_2: (64, 128) weight matrix
  b_2: (64,) bias vector
```

**Computation:**
```python
# Learned weights (showing first 2 neurons):
W_2 = np.array([
    # Neuron 0: Aggregates multiple attack signals
    [0.02, 0.01, 0.01, ..., 0.78, ..., 0.15, ..., 0.82, ...],
    #                        â†‘ neuron 42  â†‘        â†‘ neuron 91
    #                       (attack)  (weak)      (strong attack)
    
    # Neuron 1: Focuses on normal patterns
    [0.89, 0.12, 0.08, ..., 0.02, ..., 0.34, ..., 0.01, ...],
    #  â†‘ neuron 0          â†‘ ignore attack signals
    # (normal)
    
    # ... 62 more neurons
])

b_2 = np.array([0.5, 0.2, ...])  # 64 biases

# Neuron 0 (attack aggregator):
z_2[0] = sum(W_2[0] * a_1_dropout) + b_2[0]
       = 0.02*0.845 + ... + 0.78*3.456 + 0.15*0.234 + 0.82*3.821 + ... + 0.5
       = 0.017 + ... + 2.696 + 0.035 + 3.133 + ... + 0.5
       = 7.234  â† VERY STRONG! ðŸš¨ðŸš¨ðŸš¨

a_2[0] = ReLU(7.234) = 7.234

# Neuron 1 (normal pattern detector):
z_2[1] = sum(W_2[1] * a_1_dropout) + b_2[1]
       = 0.89*0.845 + 0.12*0.0 + ... + 0.02*3.456 + ... + 0.2
       = 0.752 + 0.0 + ... + 0.069 + ... + 0.2
       = 1.456  â† Moderate normal signal

a_2[1] = ReLU(1.456) = 1.456

# Full output:
a_2 = np.array([
    7.234,   # Attack aggregator ðŸš¨ðŸš¨ðŸš¨
    1.456,   # Normal signal
    5.923,   # Attack pattern 2 ðŸš¨ðŸš¨
    0.834,   # Weak normal
    6.512,   # Attack pattern 3 ðŸš¨ðŸš¨
    # ... 59 more neurons
])

print(f"Dense2 output shape: {a_2.shape}")  # (64,)
print(f"Max activation: {np.max(a_2)}")     # 7.234 (attack!)
```

### 10.5 Dense Layer 3: Final Classification (64 â†’ 2)

**Purpose:** Convert to class logits (Attack vs Benign)

**Formula:**
```python
z_3 = W_3 Â· a_2 + b_3

Where:
  W_3: (2, 64) weight matrix
  b_3: (2,) bias vector
  
  z_3[0] = logit for Benign class
  z_3[1] = logit for Attack class
```

**Computation:**
```python
# Learned weights:
W_3 = np.array([
    # Benign class neuron (prefers low attack activations)
    [0.12, 0.89, -0.23, 0.34, -0.28, ...],
    #      â†‘ high weight for normal signal (a_2[1])
    #            â†‘ negative weights for attack signals
    
    # Attack class neuron (prefers high attack activations)
    [0.95, -0.15, 0.88, 0.12, 0.92, ...],
    # â†‘ high weight for attack aggregator (a_2[0])
    #        â†‘ negative weight for normal signal
    #              â†‘ high weight for attack pattern 2
])

b_3 = np.array([-2.5, 1.8])  # Biases favor attack detection

# Benign class logit:
z_3[0] = sum(W_3[0] * a_2) + b_3[0]
       = 0.12*7.234 + 0.89*1.456 + (-0.23)*5.923 + ... - 2.5
       = 0.868 + 1.296 - 1.362 + ... - 2.5
       = -1.234  â† LOW logit for benign!

# Attack class logit:
z_3[1] = sum(W_3[1] * a_2) + b_3[1]
       = 0.95*7.234 + (-0.15)*1.456 + 0.88*5.923 + ... + 1.8
       = 6.872 - 0.218 + 5.212 + ... + 1.8
       = 15.678  â† HIGH logit for attack! ðŸš¨ðŸš¨ðŸš¨

logits = np.array([-1.234, 15.678])
print(f"Logits: {logits}")
# Logits: [-1.234, 15.678]
#          â†‘       â†‘
#        Benign  Attack (MUCH HIGHER!)
```

### 10.6 Softmax: Converting Logits to Probabilities

**Formula:**
```python
P(class_i) = exp(z_i) / sum(exp(z_j) for all j)

Range: [0, 1]
Sum: P(Benign) + P(Attack) = 1.0
```

**Computation:**
```python
import numpy as np

# Our logits:
z_benign = -1.234
z_attack = 15.678

# Compute exponentials:
exp_benign = np.exp(z_benign) = np.exp(-1.234) = 0.291
exp_attack = np.exp(z_attack) = np.exp(15.678) = 6,425,987.3

# Compute sum:
sum_exp = exp_benign + exp_attack
        = 0.291 + 6,425,987.3
        = 6,425,987.6

# Compute probabilities:
P_benign = exp_benign / sum_exp
         = 0.291 / 6,425,987.6
         = 0.0000000453
         â‰ˆ 0.0000453  (0.00453%)

P_attack = exp_attack / sum_exp
         = 6,425,987.3 / 6,425,987.6
         = 0.9999999547
         â‰ˆ 0.999955  (99.9955%)

# Final probabilities:
probabilities = np.array([0.0000453, 0.999955])

print(f"P(Benign) = {probabilities[0]:.6f} = {probabilities[0]*100:.4f}%")
# P(Benign) = 0.000045 = 0.0045%

print(f"P(Attack) = {probabilities[1]:.6f} = {probabilities[1]*100:.4f}%")
# P(Attack) = 0.999955 = 99.9955%
```

**Interpretation:**
```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CLASSIFICATION RESULT                    â”‚
â”‚                                                  â”‚
â”‚  Predicted Class: ATTACK ðŸš¨                      â”‚
â”‚  Confidence: 99.9955%                            â”‚
â”‚                                                  â”‚
â”‚  Probability Breakdown:                          â”‚
â”‚    Benign: 0.0045%  â–                            â”‚
â”‚    Attack: 99.9955% â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“   â”‚
â”‚                                                  â”‚
â”‚  The model is EXTREMELY CONFIDENT this is an    â”‚
â”‚  attack based on all accumulated evidence!      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.7 Code: Complete Dense Layers

```python
import torch
import torch.nn as nn

class DenseClassifier(nn.Module):
    def __init__(self, input_size=64, hidden_size1=128, hidden_size2=64, num_classes=2):
        super().__init__()
        
        # Dense layer 1: Expansion
        self.dense1 = nn.Linear(input_size, hidden_size1)
        self.relu1 = nn.ReLU()
        self.dropout = nn.Dropout(p=0.3)
        
        # Dense layer 2: Compression
        self.dense2 = nn.Linear(hidden_size1, hidden_size2)
        self.relu2 = nn.ReLU()
        
        # Dense layer 3: Classification
        self.dense3 = nn.Linear(hidden_size2, num_classes)
        self.softmax = nn.Softmax(dim=-1)
        
    def forward(self, x):
        """
        Args:
            x: (batch, 64) final LSTM hidden state
        
        Returns:
            logits: (batch, 2) class logits
            probs: (batch, 2) class probabilities
        """
        # Dense 1
        x = self.dense1(x)
        x = self.relu1(x)
        x = self.dropout(x)
        
        # Dense 2
        x = self.dense2(x)
        x = self.relu2(x)
        
        # Dense 3
        logits = self.dense3(x)
        probs = self.softmax(logits)
        
        return logits, probs

# Usage:
classifier = DenseClassifier(input_size=64, hidden_size1=128, hidden_size2=64, num_classes=2)

# Our final LSTM hidden state:
h_9_tensor = torch.tensor(h_9).unsqueeze(0).float()  # (1, 64)

# Forward pass:
logits, probs = classifier(h_9_tensor)

print(f"Logits: {logits}")  # [-1.234, 15.678]
print(f"Probabilities: {probs}")  # [0.000045, 0.999955]
print(f"Predicted class: {torch.argmax(probs, dim=-1)}")  # 1 (Attack)
```

### 10.8 Summary: Dense Layers Complete

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DENSE LAYERS CLASSIFICATION COMPLETE          â”‚
â”‚                                                       â”‚
â”‚  Input:  h_9 (64,) from Quantum LSTM                  â”‚
â”‚          Strong negative attack signals               â”‚
â”‚                                                       â”‚
â”‚  Processing:                                          â”‚
â”‚    Dense1: (64 â†’ 128) expand features                â”‚
â”‚    ReLU + Dropout: regularize                         â”‚
â”‚    Dense2: (128 â†’ 64) compress to key patterns       â”‚
â”‚    ReLU: activate                                     â”‚
â”‚    Dense3: (64 â†’ 2) project to class logits          â”‚
â”‚    Softmax: convert to probabilities                  â”‚
â”‚                                                       â”‚
â”‚  Output:                                              â”‚
â”‚    Logits: [-1.234, 15.678]                           â”‚
â”‚    Probs: [0.0045%, 99.9955%]                         â”‚
â”‚                                                       â”‚
â”‚  FINAL PREDICTION: ATTACK with 99.9955% confidence!   â”‚
â”‚                                                       â”‚
â”‚  âœ“ Classification complete! But wait...              â”‚
â”‚    Let's add uncertainty quantification! â†’           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## SECTION 11: BAYESIAN REASONING - UNCERTAINTY QUANTIFICATION ðŸŽ²

### 11.1 Why Bayesian Reasoning?

**Problem:** Our model said 99.9955% attack, but:
- What if we're wrong?
- How CONFIDENT should we be?
- Are there edge cases?

**Bayesian Solution:** Quantify uncertainty with probability distributions!

### 11.2 Bayesian Neural Network: Monte Carlo Dropout

**Key Idea:** Run the model MULTIPLE TIMES with different dropout masks to estimate uncertainty!

**Formula:**
```python
For N forward passes (N=100):
  predictions = []
  for i in range(N):
    enable_dropout()  # Random dropout mask each time
    pred_i = model(x)
    predictions.append(pred_i)
  
  # Compute statistics:
  mean_pred = mean(predictions)
  std_pred = std(predictions)  â† UNCERTAINTY!
```

### 11.3 Running 100 Monte Carlo Samples

```python
import torch

# Enable dropout during inference
classifier.train()  # Enable dropout!

# Run 100 forward passes:
num_samples = 100
predictions = []

for i in range(num_samples):
    logits, probs = classifier(h_9_tensor)
    predictions.append(probs.detach().numpy())

# Convert to array
predictions = np.array(predictions)  # Shape: (100, 1, 2)
predictions = predictions.squeeze(1)  # Shape: (100, 2)

print(f"Predictions shape: {predictions.shape}")  # (100, 2)
```

### 11.4 Analyzing the Distribution

**Benign class probabilities (100 samples):**
```python
benign_probs = predictions[:, 0]

print(f"Mean: {np.mean(benign_probs):.6f}")  # 0.000048
print(f"Std:  {np.std(benign_probs):.6f}")   # 0.000012
print(f"Min:  {np.min(benign_probs):.6f}")   # 0.000023
print(f"Max:  {np.max(benign_probs):.6f}")   # 0.000089

# Visualization:
import matplotlib.pyplot as plt
plt.hist(benign_probs, bins=20, color='blue', alpha=0.7)
plt.xlabel('P(Benign)')
plt.ylabel('Frequency')
plt.title('Bayesian Uncertainty: Benign Class')
plt.axvline(np.mean(benign_probs), color='red', linestyle='--', label='Mean')
plt.legend()
# Shows tight distribution near 0 â†’ HIGH CERTAINTY!
```

**Attack class probabilities (100 samples):**
```python
attack_probs = predictions[:, 1]

print(f"Mean: {np.mean(attack_probs):.6f}")  # 0.999952
print(f"Std:  {np.std(attack_probs):.6f}")   # 0.000012
print(f"Min:  {np.min(attack_probs):.6f}")   # 0.999911
print(f"Max:  {np.max(attack_probs):.6f}")   # 0.999977

# Visualization:
plt.hist(attack_probs, bins=20, color='red', alpha=0.7)
plt.xlabel('P(Attack)')
plt.ylabel('Frequency')
plt.title('Bayesian Uncertainty: Attack Class')
plt.axvline(np.mean(attack_probs), color='blue', linestyle='--', label='Mean')
plt.legend()
# Shows tight distribution near 1.0 â†’ HIGH CERTAINTY! ðŸš¨
```

### 11.5 Uncertainty Metrics

**Entropy (information uncertainty):**
```python
# Formula: H = -sum(p * log(p))
def entropy(probs):
    # Clip to avoid log(0)
    p_safe = np.clip(probs, 1e-10, 1.0)
    return -np.sum(probs * np.log(p_safe), axis=-1)

# Mean prediction:
mean_pred = np.mean(predictions, axis=0)  # [0.000048, 0.999952]

H = entropy(mean_pred)
print(f"Entropy: {H:.6f}")  # 0.000332

# Interpretation:
# H = 0      â†’ Perfect certainty (one class has P=1.0)
# H = log(2) â‰ˆ 0.693 â†’ Maximum uncertainty (P=0.5 for both)
# H = 0.000332 â†’ VERY LOW uncertainty! ðŸŽ¯
```

**Predictive Variance:**
```python
# Variance across samples
attack_var = np.var(attack_probs)
print(f"Attack probability variance: {attack_var:.10f}")
# Output: 0.0000000144

# Very low variance â†’ Model is CONSISTENT! ðŸŽ¯
```

**Confidence Interval (95%):**
```python
# Mean Â± 1.96 * std (95% CI)
attack_mean = np.mean(attack_probs)
attack_std = np.std(attack_probs)

ci_lower = attack_mean - 1.96 * attack_std
ci_upper = attack_mean + 1.96 * attack_std

print(f"Attack probability: {attack_mean:.6f}")
print(f"95% CI: [{ci_lower:.6f}, {ci_upper:.6f}]")
# Output: 
#   Attack probability: 0.999952
#   95% CI: [0.999928, 0.999976]

# Interpretation: We're 95% confident P(Attack) âˆˆ [0.9999, 1.0] ðŸš¨
```

### 11.6 Bayesian Decision Making

**Risk Assessment:**
```python
# Define risk thresholds:
HIGH_CONFIDENCE_THRESHOLD = 0.95  # 95% certainty
LOW_UNCERTAINTY_THRESHOLD = 0.01  # 1% std

# Check our prediction:
is_high_confidence = attack_mean > HIGH_CONFIDENCE_THRESHOLD
is_low_uncertainty = attack_std < LOW_UNCERTAINTY_THRESHOLD

if is_high_confidence and is_low_uncertainty:
    decision = "BLOCK IMMEDIATELY! ðŸš¨"
    risk_level = "CRITICAL"
elif is_high_confidence:
    decision = "BLOCK with human review"
    risk_level = "HIGH"
else:
    decision = "Flag for investigation"
    risk_level = "MEDIUM"

print(f"Decision: {decision}")
print(f"Risk Level: {risk_level}")
# Output:
#   Decision: BLOCK IMMEDIATELY! ðŸš¨
#   Risk Level: CRITICAL
```

### 11.7 Comparison: With vs Without Bayesian Reasoning

**Without Bayesian:**
```python
Prediction: Attack
Confidence: 99.9955%

Question: But HOW sure are you?
Answer: I don't know! ðŸ¤·
```

**With Bayesian:**
```python
Prediction: Attack
Confidence: 99.9952% Â± 0.0012%
Uncertainty: Very low (H=0.000332)
95% CI: [0.9999, 1.0]

Question: But HOW sure are you?
Answer: 100 different runs all agree! ðŸŽ¯
```

### 11.8 Code: Bayesian Inference Module

```python
class BayesianPredictor:
    def __init__(self, model, num_samples=100):
        self.model = model
        self.num_samples = num_samples
        
    def predict_with_uncertainty(self, x):
        """
        Args:
            x: (batch, input_size) input features
        
        Returns:
            mean_pred: (batch, num_classes) mean prediction
            std_pred: (batch, num_classes) uncertainty
            entropy: (batch,) prediction entropy
        """
        # Enable dropout
        self.model.train()
        
        # Collect predictions
        predictions = []
        for _ in range(self.num_samples):
            _, probs = self.model(x)
            predictions.append(probs.detach().numpy())
        
        predictions = np.array(predictions)  # (num_samples, batch, classes)
        
        # Compute statistics
        mean_pred = np.mean(predictions, axis=0)
        std_pred = np.std(predictions, axis=0)
        
        # Compute entropy
        entropy = -np.sum(mean_pred * np.log(np.clip(mean_pred, 1e-10, 1.0)), axis=-1)
        
        return mean_pred, std_pred, entropy

# Usage:
bayesian = BayesianPredictor(classifier, num_samples=100)
mean_pred, std_pred, entropy = bayesian.predict_with_uncertainty(h_9_tensor)

print(f"Mean prediction: {mean_pred}")
# [[0.000048, 0.999952]]

print(f"Uncertainty (std): {std_pred}")
# [[0.000012, 0.000012]]

print(f"Entropy: {entropy}")
# [0.000332]

# Final decision:
predicted_class = np.argmax(mean_pred, axis=-1)[0]
confidence = mean_pred[0, predicted_class]
uncertainty = std_pred[0, predicted_class]

print(f"\nðŸŽ¯ FINAL BAYESIAN PREDICTION:")
print(f"Class: {'ATTACK' if predicted_class == 1 else 'BENIGN'}")
print(f"Confidence: {confidence:.4%} Â± {uncertainty:.4%}")
print(f"Uncertainty: {'LOW' if entropy[0] < 0.1 else 'HIGH'}")
```

### 11.9 Summary: Bayesian Reasoning Complete

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      BAYESIAN UNCERTAINTY QUANTIFICATION COMPLETE      â”‚
â”‚                                                        â”‚
â”‚  Method: Monte Carlo Dropout (100 samples)             â”‚
â”‚                                                        â”‚
â”‚  Results:                                              â”‚
â”‚    Mean P(Attack): 99.9952%                            â”‚
â”‚    Std P(Attack):  Â±0.0012%                            â”‚
â”‚    95% CI: [99.9928%, 99.9976%]                        â”‚
â”‚    Entropy: 0.000332 (very low!)                       â”‚
â”‚                                                        â”‚
â”‚  Interpretation:                                       â”‚
â”‚    âœ“ HIGH confidence (>95%)                            â”‚
â”‚    âœ“ LOW uncertainty (<1%)                             â”‚
â”‚    âœ“ CONSISTENT across 100 runs                        â”‚
â”‚                                                        â”‚
â”‚  Decision: BLOCK IMMEDIATELY! ðŸš¨                       â”‚
â”‚  Risk Level: CRITICAL                                  â”‚
â”‚                                                        â”‚
â”‚  âœ“ Now let's add temporal forecasting! â†’              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## SECTION 12: TEMPORAL REASONING - WHAT HAPPENS NEXT? ðŸ”®

### 12.1 Hidden Markov Model (HMM): Predicting Future States

**Question:** We detected an attack NOW, but what will happen NEXT?

**Temporal HMM:** Model the evolution of attack states over time!

### 12.2 HMM States: Attack Progression

**Define 4 hidden states:**
```python
State 0: NORMAL      (no attack)
State 1: SUSPICIOUS  (probe/reconnaissance)
State 2: ATTACKING   (active attack)
State 3: CRITICAL    (severe attack)
```

**Our port scan evolution:**
```python
t=0-2: SUSPICIOUS  (count=10-50, serror=0.80-0.88)
t=3-5: ATTACKING   (count=75-150, serror=0.90-0.94)
t=6-9: CRITICAL    (count=200-511, serror=0.96-1.00)
```

### 12.3 HMM Parameters (Learned from Data)

**Initial probabilities Ï€:**
```python
# P(start in each state)
pi = np.array([0.85, 0.10, 0.04, 0.01])
#               â†‘     â†‘     â†‘     â†‘
#            Normal Susp Attack Crit

# Most traffic starts NORMAL (85%)
```

**Transition probabilities A:**
```python
# A[i,j] = P(state j at t+1 | state i at t)
A = np.array([
    # From:  To: Normal  Susp  Attack  Crit
    [0.95,  0.04,  0.01,  0.00],  # Normal â†’ mostly stays normal
    [0.20,  0.50,  0.25,  0.05],  # Suspicious â†’ can escalate
    [0.05,  0.10,  0.70,  0.15],  # Attack â†’ persists or worsens
    [0.02,  0.03,  0.20,  0.75],  # Critical â†’ hard to recover
])

# Example: If currently in Attack state (row 2):
#   70% chance stays in Attack
#   15% chance escalates to Critical! ðŸš¨
#   10% chance drops to Suspicious
#   5% chance returns to Normal
```

**Emission probabilities B:**
```python
# B[i,k] = P(observe class k | in state i)
B = np.array([
    # State:   P(Benign)  P(Attack)
    [0.95,      0.05],      # Normal â†’ 95% benign
    [0.60,      0.40],      # Suspicious â†’ 40% attacks
    [0.10,      0.90],      # Attack â†’ 90% attacks
    [0.01,      0.99],      # Critical â†’ 99% attacks! ðŸš¨
])
```

### 12.4 Viterbi Algorithm: Most Likely State Sequence

**Given observations:** Our 10 predictions over time

```python
# Observations (from our model):
observations = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
#                        â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘
#                       All classified as ATTACK (class 1)

# Run Viterbi to find most likely hidden state sequence
from hmmlearn import hmm

model = hmm.MultinomialHMM(n_components=4)
model.startprob_ = pi
model.transmat_ = A
model.emissionprob_ = B

# Decode
states = model.predict(observations.reshape(-1, 1))
print(f"Most likely state sequence: {states}")
# Output: [1, 1, 2, 2, 2, 3, 3, 3, 3, 3]
#          â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘
#         Suspâ†’Suspâ†’Attackâ†’Attackâ†’Critical!
```

**Interpretation:**
```python
Timeline of Attack Evolution:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
t=0-1: SUSPICIOUS (State 1)
  - Initial probing
  - count=10-25, serror=0.80-0.84
  - Model detects something odd

t=2-5: ATTACKING (State 2)
  - Active scanning
  - count=50-150, serror=0.88-0.94
  - Model confirms attack

t=6-9: CRITICAL (State 3)
  - Full-scale port scan!
  - count=200-511, serror=0.96-1.00
  - Maximum threat level! ðŸš¨ðŸš¨ðŸš¨
```

### 12.5 Forward Algorithm: Probability of Being in Each State

```python
# Compute forward probabilities (probability of being in each state at each time)
from scipy.special import logsumexp

def forward_algorithm(observations, pi, A, B):
    T = len(observations)
    N = len(pi)
    
    # Initialize
    alpha = np.zeros((T, N))
    alpha[0] = pi * B[:, observations[0]]
    
    # Iterate
    for t in range(1, T):
        for j in range(N):
            alpha[t, j] = np.sum(alpha[t-1] * A[:, j]) * B[j, observations[t]]
    
    return alpha

alpha = forward_algorithm(observations, pi, A, B)

# At final time step (t=9):
print(f"P(Normal | obs): {alpha[9, 0]:.6f}")      # 0.000001
print(f"P(Suspicious | obs): {alpha[9, 1]:.6f}") # 0.000023
print(f"P(Attack | obs): {alpha[9, 2]:.6f}")     # 0.012456
print(f"P(Critical | obs): {alpha[9, 3]:.6f}")   # 0.987520 ðŸš¨

# 98.75% probability we're in CRITICAL state!
```

### 12.6 Forecasting: What Happens at t=10?

**Predict next state probabilities:**
```python
# Current state distribution (at t=9):
current_dist = alpha[9] / np.sum(alpha[9])
# [0.000001, 0.000023, 0.012456, 0.987520]

# Predict next state (t=10):
next_dist = current_dist @ A  # Matrix multiplication

print(f"Predicted state distribution at t=10:")
print(f"  P(Normal):     {next_dist[0]:.4f}")  # 0.0197
print(f"  P(Suspicious): {next_dist[1]:.4f}")  # 0.0296
print(f"  P(Attack):     {next_dist[2]:.4f}")  # 0.1975
print(f"  P(Critical):   {next_dist[3]:.4f}")  # 0.7532 ðŸš¨

# Interpretation: 75.32% chance attack stays CRITICAL!
```

**Predict t=11, t=12, ..., t=15:**
```python
forecast = [current_dist]
for _ in range(5):
    next_dist = forecast[-1] @ A
    forecast.append(next_dist)

forecast = np.array(forecast)

# Visualize:
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
for state in range(4):
    plt.plot(range(10, 16), forecast[1:, state], 
             marker='o', label=f'State {state}')
plt.xlabel('Time Step')
plt.ylabel('Probability')
plt.title('HMM Forecast: Attack State Evolution')
plt.legend(['Normal', 'Suspicious', 'Attack', 'Critical'])
plt.grid(True)

# Shows: Critical state probability remains high (>70%) for next 5 steps! ðŸš¨
```

### 12.7 Attack Duration Estimation

**How long will this attack last?**
```python
# Expected time to exit Critical state:
# Given transition probabilities from Critical:
P_stay_critical = A[3, 3]  # 0.75

# Expected duration = 1 / (1 - P_stay)
expected_duration = 1 / (1 - P_stay_critical)
print(f"Expected attack duration: {expected_duration:.1f} time steps")
# Output: 4.0 time steps

# Interpretation: Attack will likely persist for 4 more time steps!
```

### 12.8 Summary: Temporal Reasoning Complete

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         TEMPORAL HMM REASONING COMPLETE                â”‚
â”‚                                                        â”‚
â”‚  Method: Hidden Markov Model (4 states)                â”‚
â”‚                                                        â”‚
â”‚  Current State (t=9):                                  â”‚
â”‚    Most likely: CRITICAL (State 3)                     â”‚
â”‚    Probability: 98.75%                                 â”‚
â”‚                                                        â”‚
â”‚  Forecast (t=10-15):                                   â”‚
â”‚    P(Critical) remains >70% for next 5 steps! ðŸš¨       â”‚
â”‚    Attack is PERSISTENT                                â”‚
â”‚                                                        â”‚
â”‚  Expected Duration:                                    â”‚
â”‚    4.0 more time steps before mitigation               â”‚
â”‚                                                        â”‚
â”‚  Recommended Action:                                   â”‚
â”‚    IMMEDIATE BLOCKING + SUSTAINED MONITORING           â”‚
â”‚                                                        â”‚
â”‚  âœ“ All processing complete! â†’ Final summary...        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**STATUS: Sections 10-12 Complete (Dense Layers, Bayesian, Temporal)!**  
Next: Final Summary & Complete Data Flow Recap!

