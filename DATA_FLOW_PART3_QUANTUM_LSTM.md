# Part 3: Quantum LSTM - Temporal Pattern Recognition ğŸ•°ï¸

## SECTION 7: LSTM FUNDAMENTALS & QUANTUM GATES ğŸ”„

### 7.1 Why LSTM? The Temporal Challenge

**Problem:** Our port scan evolves over time!

```python
t-9: count=10,  serror=0.80  â† Early stage
t-5: count=100, serror=0.92  â† Ramping up
t-1: count=400, serror=0.99  â† Intensifying
t=0: count=511, serror=1.00  â† PEAK ATTACK!

Pattern: Exponential growth + increasing errors
```

**Classical RNN Problem:** Vanishing gradients - can't remember early signals!

**LSTM Solution:** Gated memory cells - remember important patterns across time!

### 7.2 Classical LSTM Gates (Quick Recap)

```python
# At each time step t:
f_t = sigmoid(W_f Â· [h_{t-1}, x_t] + b_f)  # Forget gate (0-1)
i_t = sigmoid(W_i Â· [h_{t-1}, x_t] + b_i)  # Input gate (0-1)
o_t = sigmoid(W_o Â· [h_{t-1}, x_t] + b_o)  # Output gate (0-1)
g_t = tanh(W_g Â· [h_{t-1}, x_t] + b_g)     # Candidate memory

# Update cell state:
c_t = f_t âŠ™ c_{t-1} + i_t âŠ™ g_t

# Output hidden state:
h_t = o_t âŠ™ tanh(c_t)
```

**Gates decide:**
- **Forget:** What old info to discard (f_t)
- **Input:** What new info to store (i_t)
- **Output:** What info to expose (o_t)

### 7.3 Quantum LSTM: Key Differences

**Classical LSTM:** Matrix multiplications + sigmoids  
**Quantum LSTM:** Quantum gates + measurements!

```python
Classical:  WÂ·x + b  (linear transformation)
            â†“
Quantum:    U(Î¸)|xâŸ©  (unitary rotation)

Classical:  sigmoid(z)  (activation)
            â†“
Quantum:    Measure Pauli-Z  (expectation)
```

**Our Quantum LSTM Architecture:**
```python
Input:  Quantum features from CNN: Q = (10, 80)
        80 quantum expectations at each time step

Gates:  Quantum parameterized circuits
        - Forget gate: U_f(Î¸_f)
        - Input gate:  U_i(Î¸_i)
        - Output gate: U_o(Î¸_o)
        - Candidate:   U_g(Î¸_g)

Output: Hidden states: h_0, h_1, ..., h_9
        Cell states:   c_0, c_1, ..., c_9
```

### 7.4 Quantum Gate Design: Parameterized Circuit

**Single gate structure (e.g., Forget Gate):**
```python
4 qubits: q0, q1, q2, q3

Layer 1: Encode input
  RY(Î¸_0^input)|0âŸ© â†’ q0
  RY(Î¸_1^input)|0âŸ© â†’ q1
  RY(Î¸_2^input)|0âŸ© â†’ q2
  RY(Î¸_3^input)|0âŸ© â†’ q3

Layer 2: Learnable rotations
  RY(Î¸_0^learn) â†’ q0
  RY(Î¸_1^learn) â†’ q1
  RY(Î¸_2^learn) â†’ q2
  RY(Î¸_3^learn) â†’ q3

Layer 3: Entanglement
  CNOT(q0, q1)
  CNOT(q1, q2)
  CNOT(q2, q3)

Layer 4: Final rotations
  RZ(Ï†_0) â†’ q0
  RZ(Ï†_1) â†’ q1
  RZ(Ï†_2) â†’ q2
  RZ(Ï†_3) â†’ q3

Measure: âŸ¨Z_0âŸ©, âŸ¨Z_1âŸ©, âŸ¨Z_2âŸ©, âŸ¨Z_3âŸ©
```

### 7.5 Initial State (t=-1, Before First Time Step)

```python
# Initialize hidden state and cell state
h_prev = np.zeros(64)  # 64 hidden units (16 groups Ã— 4 qubits)
c_prev = np.zeros(64)  # 64 cell state units

print(f"Initial hidden state shape: {h_prev.shape}")  # (64,)
print(f"Initial cell state shape: {c_prev.shape}")    # (64,)
print(f"All values: {h_prev[0]}")                     # 0.0

# This represents "no memory" at the start
```

### 7.6 Input Preparation for First Time Step (t=0)

**From Quantum CNN:** Q[0] = first time step quantum features

```python
# Our quantum features at t=0 (first 8 shown):
q_t0 = [+0.23, -0.67, -0.91, -0.88, -0.51, +0.35, -0.73, +0.08, ...]
        â†‘      â†‘      â†‘      â†‘      â†‘      â†‘      â†‘      â†‘
       dur   count  serror  hosts  src_b  dst_b   SYN   ACK

# Full shape:
print(f"Quantum features at t=0: {len(q_t0)}")  # 80 features

# Concatenate with previous hidden state
lstm_input = np.concatenate([h_prev, q_t0])
print(f"LSTM input shape: {lstm_input.shape}")  # (144,) = 64 + 80
```

### 7.7 Code: Quantum Gate Implementation

```python
import pennylane as qml

# Quantum device
dev = qml.device('default.qubit', wires=4)

@qml.qnode(dev)
def quantum_gate(x_input, h_prev_part, params):
    """
    Single quantum LSTM gate
    
    Args:
        x_input: 4 quantum features from current time
        h_prev_part: 4 values from previous hidden state
        params: Learnable parameters [Î¸_learn, Ï†_learn]
    
    Returns:
        4 expectation values
    """
    # Layer 1: Encode current input (4 features â†’ 4 qubits)
    for i in range(4):
        theta_input = np.pi * (x_input[i] + 1) / 2  # Map [-1,1] â†’ [0,Ï€]
        qml.RY(theta_input, wires=i)
    
    # Layer 2: Encode previous hidden state (adds memory)
    for i in range(4):
        theta_hidden = np.pi * (h_prev_part[i] + 1) / 2
        qml.RY(theta_hidden, wires=i)
    
    # Layer 3: Learnable rotations (trained parameters)
    for i in range(4):
        qml.RY(params['theta'][i], wires=i)
    
    # Layer 4: Entanglement (create correlations)
    qml.CNOT(wires=[0, 1])
    qml.CNOT(wires=[1, 2])
    qml.CNOT(wires=[2, 3])
    
    # Layer 5: Final learnable rotations
    for i in range(4):
        qml.RZ(params['phi'][i], wires=i)
    
    # Measure expectations
    return [qml.expval(qml.PauliZ(i)) for i in range(4)]

# Example: Compute forget gate at t=0
x_input = q_t0[0:4]      # First 4 quantum features
h_prev_part = h_prev[0:4]  # First 4 hidden state values

# Learned parameters (from training):
forget_params = {
    'theta': [0.45, 0.67, 0.89, 0.34],
    'phi': [0.12, -0.23, 0.56, -0.78]
}

forget_output = quantum_gate(x_input, h_prev_part, forget_params)
print(f"Forget gate output: {forget_output}")
# Output: [0.34, -0.12, 0.67, -0.89]
```

### 7.8 Summary: LSTM Setup Complete

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         QUANTUM LSTM INITIALIZATION                  â”‚
â”‚                                                      â”‚
â”‚  Input:  80 quantum features per time step          â”‚
â”‚  Memory: h_{-1} = zeros(64), c_{-1} = zeros(64)     â”‚
â”‚                                                      â”‚
â”‚  Gates:  4 quantum circuits (forget, input, output, candidate) â”‚
â”‚          Each processes 4 qubits at a time          â”‚
â”‚          16 groups total (64 hidden units)          â”‚
â”‚                                                      â”‚
â”‚  Ready:  Process 10 time steps sequentially!        â”‚
â”‚                                                      â”‚
â”‚  âœ“ Starting temporal processing at t=0...           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## SECTION 8: QUANTUM LSTM TIME STEP t=0 (DETAILED WALKTHROUGH) â±ï¸

### 8.1 Input at t=0

```python
# Quantum features from CNN (showing first 8 of 80):
q_0 = [+0.23, -0.67, -0.91, -0.88, -0.51, +0.35, -0.73, +0.08, ...]

# Previous memory (all zeros on first step):
h_{-1} = [0.0, 0.0, 0.0, ..., 0.0]  # 64 values
c_{-1} = [0.0, 0.0, 0.0, ..., 0.0]  # 64 values

# Combined input to gates:
gate_input = concat([h_{-1}, q_0]) = [0.0, ..., 0.0, +0.23, -0.67, ...]
                                      â””â”€ 64 zeros â”€â”˜ â””â”€â”€â”€â”€ 80 quantum â”€â”€â”€â”€â”˜
```

### 8.2 Forget Gate: f_0 (What to Remember?)

**Purpose:** Decide what old information to keep from c_{-1}

**Process:** Apply quantum circuit to compute forget gate values

```python
# Split input into groups of 4 for quantum processing
f_0_values = []

for group in range(16):  # 64 hidden units / 4 qubits = 16 groups
    # Get relevant parts
    h_part = h_{-1}[group*4 : (group+1)*4]  # 4 previous hidden values
    q_part = q_0[group*5 : (group+1)*5][:4]  # 4 quantum features (with overlap)
    
    # Apply quantum forget gate
    f_group = quantum_gate(q_part, h_part, forget_params[group])
    f_0_values.extend(f_group)

f_0 = np.array(f_0_values)
print(f"Forget gate shape: {f_0.shape}")  # (64,)

# First 8 values (example):
print(f"f_0[0:8] = {f_0[0:8]}")
# Output: [0.02, -0.01, 0.03, -0.04, 0.01, 0.05, -0.02, 0.00]
#          â†‘ Near 0 = forget old info (good, since c_{-1} is all zeros anyway)
```

**Interpretation:**
```python
f_0 values near 0 â†’ Forget previous cell state
f_0 values near +1 â†’ Keep previous cell state
f_0 values near -1 â†’ Strongly forget

Since c_{-1} = 0 (no previous memory), f_0 values don't matter much yet!
```

### 8.3 Input Gate: i_0 (What to Store?)

**Purpose:** Decide what new information to store in cell state

```python
# Same process, different learned parameters
i_0_values = []

for group in range(16):
    h_part = h_{-1}[group*4 : (group+1)*4]
    q_part = q_0[group*5 : (group+1)*5][:4]
    
    # Apply quantum input gate
    i_group = quantum_gate(q_part, h_part, input_params[group])
    i_0_values.extend(i_group)

i_0 = np.array(i_0_values)

# First 8 values (example):
print(f"i_0[0:8] = {i_0[0:8]}")
# Output: [0.67, 0.89, 0.92, 0.85, 0.71, 0.45, 0.88, 0.34]
#          â†‘ Positive values = STORE new info (attack patterns!)
```

**Interpretation:**
```python
High i_0 values (0.67-0.92) at positions 0-4 and 6
â†’ STORE information about:
  - count (high connections)
  - serror_rate (100% errors)
  - SYN flag (attack pattern)

These are KEY ATTACK INDICATORS! ğŸš¨
```

### 8.4 Candidate Memory: g_0 (What to Potentially Add?)

**Purpose:** Compute candidate values to add to cell state

```python
# Apply quantum candidate gate
g_0_values = []

for group in range(16):
    h_part = h_{-1}[group*4 : (group+1)*4]
    q_part = q_0[group*5 : (group+1)*5][:4]
    
    # Apply quantum candidate gate (uses tanh-like output)
    g_group = quantum_gate(q_part, h_part, candidate_params[group])
    g_0_values.extend(g_group)

g_0 = np.array(g_0_values)

# First 8 values (example):
print(f"g_0[0:8] = {g_0[0:8]}")
# Output: [0.34, -0.78, -0.95, -0.91, -0.62, 0.28, -0.81, 0.15]
#          â†‘      â†‘      â†‘      â†‘      â†‘            â†‘
#         ok   ATTACK! ATTACK! ATTACK! high      ATTACK!
```

**Interpretation:**
```python
Negative g_0 values = Attack patterns detected!
  g_0[1] = -0.78  â† High connection count
  g_0[2] = -0.95  â† 100% SYN errors (EXTREME!)
  g_0[3] = -0.91  â† 255 hosts scanned
  g_0[6] = -0.81  â† SYN flag pattern

Positive g_0 values = Normal patterns
  g_0[0] = +0.34  â† Duration not suspicious
  g_0[5] = +0.28  â† Normal destination bytes
```

### 8.5 Update Cell State: c_0

**Formula:** c_t = f_t âŠ™ c_{t-1} + i_t âŠ™ g_t

```python
# Element-wise computation:
c_0 = f_0 * c_{-1} + i_0 * g_0

# Let's compute first 4 values explicitly:
c_0[0] = f_0[0] * c_{-1}[0] + i_0[0] * g_0[0]
       = 0.02 * 0.0 + 0.67 * 0.34
       = 0.0 + 0.228
       = 0.228

c_0[1] = f_0[1] * c_{-1}[1] + i_0[1] * g_0[1]
       = -0.01 * 0.0 + 0.89 * (-0.78)
       = 0.0 - 0.694
       = -0.694  â† STORED ATTACK SIGNAL! ğŸš¨

c_0[2] = f_0[2] * c_{-1}[2] + i_0[2] * g_0[2]
       = 0.03 * 0.0 + 0.92 * (-0.95)
       = 0.0 - 0.874
       = -0.874  â† STRONG ATTACK SIGNAL! ğŸš¨ğŸš¨

c_0[3] = f_0[3] * c_{-1}[3] + i_0[3] * g_0[3]
       = -0.04 * 0.0 + 0.85 * (-0.91)
       = 0.0 - 0.774
       = -0.774  â† ATTACK PATTERN STORED! ğŸš¨ğŸš¨

# Full cell state:
print(f"c_0[0:8] = {c_0[0:8]}")
# Output: [0.228, -0.694, -0.874, -0.774, -0.440, 0.126, -0.774, 0.051]
```

**Interpretation:**
```python
c_0 = LSTM MEMORY after first time step

Negative values (cells 1, 2, 3, 4, 6) = ATTACK PATTERNS IN MEMORY
  c_0[2] = -0.874  â† Strongest: 100% SYN errors
  c_0[3] = -0.774  â† Strong: 255 hosts scanned
  c_0[6] = -0.774  â† Strong: SYN flag pattern

Positive values (cells 0, 5, 7) = NORMAL PATTERNS
  c_0[0] = 0.228   â† Duration okay
  c_0[5] = 0.126   â† Bytes okay

The LSTM has LEARNED to store attack indicators! ğŸ¯
```

### 8.6 Output Gate: o_0 (What to Expose?)

**Purpose:** Decide what information to output from cell state

```python
# Apply quantum output gate
o_0_values = []

for group in range(16):
    h_part = h_{-1}[group*4 : (group+1)*4]
    q_part = q_0[group*5 : (group+1)*5][:4]
    
    # Apply quantum output gate
    o_group = quantum_gate(q_part, h_part, output_params[group])
    o_0_values.extend(o_group)

o_0 = np.array(o_0_values)

# First 8 values (example):
print(f"o_0[0:8] = {o_0[0:8]}")
# Output: [0.45, 0.89, 0.91, 0.87, 0.78, 0.34, 0.88, 0.23]
#          â†‘     â†‘     â†‘     â†‘     â†‘            â†‘
#         med  HIGH  HIGH  HIGH  HIGH         HIGH

# High output gate values = EXPOSE this information!
```

### 8.7 Hidden State: h_0 (Final Output)

**Formula:** h_t = o_t âŠ™ tanh(c_t)

```python
# Apply tanh to cell state (squash to [-1, 1])
c_0_tanh = np.tanh(c_0)

# Element-wise multiply with output gate
h_0 = o_0 * c_0_tanh

# First 4 values explicitly:
h_0[0] = o_0[0] * tanh(c_0[0])
       = 0.45 * tanh(0.228)
       = 0.45 * 0.224
       = 0.101

h_0[1] = o_0[1] * tanh(c_0[1])
       = 0.89 * tanh(-0.694)
       = 0.89 * (-0.600)
       = -0.534  â† ATTACK SIGNAL EXPOSED! ğŸš¨

h_0[2] = o_0[2] * tanh(c_0[2])
       = 0.91 * tanh(-0.874)
       = 0.91 * (-0.704)
       = -0.641  â† STRONG ATTACK SIGNAL! ğŸš¨ğŸš¨

h_0[3] = o_0[3] * tanh(c_0[3])
       = 0.87 * tanh(-0.774)
       = 0.87 * (-0.651)
       = -0.566  â† ATTACK EXPOSED! ğŸš¨

# Full hidden state:
print(f"h_0 shape: {h_0.shape}")  # (64,)
print(f"h_0[0:8] = {h_0[0:8]}")
# Output: [0.101, -0.534, -0.641, -0.566, -0.343, 0.043, -0.641, 0.012]
```

**Interpretation:**
```python
h_0 = LSTM OUTPUT at t=0

Strong negative values = ATTACK DETECTED!
  h_0[2] = -0.641  â† 100% SYN errors (CRITICAL!)
  h_0[3] = -0.566  â† Full subnet scan
  h_0[6] = -0.641  â† SYN flag pattern

Weak positive values = Normal activity
  h_0[0] = 0.101   â† Duration normal
  h_0[5] = 0.043   â† Bytes normal

The LSTM is SHOUTING "ATTACK!" with negative values! ğŸ“¢ğŸš¨
```

### 8.8 Summary: First Time Step Complete

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         QUANTUM LSTM - TIME STEP t=0 COMPLETE      â”‚
â”‚                                                    â”‚
â”‚  Input:  q_0 (80 quantum features from CNN)        â”‚
â”‚          h_{-1} = 0 (no previous hidden state)     â”‚
â”‚          c_{-1} = 0 (no previous cell state)       â”‚
â”‚                                                    â”‚
â”‚  Gates:                                            â”‚
â”‚    f_0 â‰ˆ 0      â†’ Forget nothing (no old memory)  â”‚
â”‚    i_0 = HIGH   â†’ Store NEW attack patterns!      â”‚
â”‚    g_0 = NEG    â†’ Attack candidates detected      â”‚
â”‚    o_0 = HIGH   â†’ Expose the attack signals       â”‚
â”‚                                                    â”‚
â”‚  Output:                                           â”‚
â”‚    c_0: [-0.874, -0.774, -0.694, ...]  â† Memory   â”‚
â”‚    h_0: [-0.641, -0.566, -0.534, ...]  â† Output   â”‚
â”‚                                                    â”‚
â”‚  Status: ATTACK PATTERNS STORED AND EXPOSED! ğŸš¨   â”‚
â”‚                                                    â”‚
â”‚  âœ“ Ready for next time step (t=1)...              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## SECTION 9: TEMPORAL EVOLUTION (t=1 through t=9) ğŸ“ˆ

### 9.1 The Pattern Across Time

Now let's see how the LSTM tracks the EVOLUTION of the attack!

**Recall our attack timeline:**
```python
t=0: count=10,  serror=0.80  â† Starting
t=1: count=25,  serror=0.84
t=2: count=50,  serror=0.88
...
t=8: count=400, serror=0.99
t=9: count=511, serror=1.00  â† PEAK!
```

### 9.2 Time Step t=1: Attack Grows

```python
# Input at t=1:
q_1 = quantum_cnn_output[1]  # 80 quantum features
h_prev = h_0                  # Previous hidden state
c_prev = c_0                  # Previous cell state

# Forget gate:
f_1 = quantum_forget_gate([h_0, q_1])
# Output: [0.78, 0.92, 0.95, 0.89, ...]
#          â†‘ HIGH values = KEEP previous memory!

# Why? Because previous attack patterns are STILL RELEVANT!
```

**Cell state update:**
```python
c_1 = f_1 * c_0 + i_1 * g_1

# Key values:
c_1[2] = 0.95 * (-0.874) + 0.88 * (-0.82)
       = -0.830 + (-0.722)
       = -1.552  â† GROWING ATTACK SIGNAL! ğŸš¨ğŸ“ˆ

# Old attack memory (-0.874) + new attack evidence (-0.82)
# = STRONGER ATTACK SIGNAL (-1.552)!
```

**Hidden state:**
```python
h_1 = o_1 * tanh(c_1)

h_1[2] = 0.93 * tanh(-1.552)
       = 0.93 * (-0.914)
       = -0.850  â† ATTACK CONFIDENCE INCREASED!

# Compare: h_0[2] = -0.641, h_1[2] = -0.850
# Attack signal is GROWING STRONGER! ğŸ“ˆ
```

### 9.3 Time Steps t=2 through t=7: Accumulating Evidence

```python
# As we process more time steps, the LSTM accumulates evidence:

t=2: h_2[2] = -0.910  â† More confident
t=3: h_3[2] = -0.935  â† Even more
t=4: h_4[2] = -0.948  â† Very confident
t=5: h_5[2] = -0.957  â† Extremely confident
t=6: h_6[2] = -0.963  â† Nearly certain
t=7: h_7[2] = -0.971  â† Highly certain

Pattern: Monotonic increase in attack confidence! ğŸ“Š
```

**Visualization:**
```python
ATTACK CONFIDENCE OVER TIME (h_t[2] values):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 0.0  â”¤
-0.2  â”¤
-0.4  â”¤
-0.6  â”¤ â—
-0.8  â”¤   â—â—
-1.0  â”¤      â—â—â—â—â—â—â—â—â—
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      t=0 1  2  3 4 5 6 7 8 9

The LSTM is learning: "This attack is REAL and GROWING!"
```

### 9.4 Final Time Step t=9: Peak Attack

```python
# Input at t=9 (PEAK ATTACK!):
q_9 = quantum_cnn_output[9]
# Features: count=511, serror=1.00, hosts=255 (MAXIMUM!)

# Forget gate:
f_9 = [0.99, 0.98, 0.99, 0.97, ...]
# â†‘ KEEP ALL ATTACK MEMORY! Everything is relevant!

# Input gate:
i_9 = [0.95, 0.98, 0.99, 0.96, ...]
# â†‘ STORE THIS PEAK ATTACK INFO!

# Candidate:
g_9 = [0.12, -0.99, -1.00, -0.98, ...]
#            â†‘      â†‘      â†‘
#          EXTREME ATTACK VALUES!

# Cell state update:
c_9[2] = 0.99 * c_8[2] + 0.99 * (-1.00)
       = 0.99 * (-1.845) + 0.99 * (-1.00)
       = -1.827 - 0.990
       = -2.817  â† MAXIMUM ATTACK SIGNAL! ğŸš¨ğŸš¨ğŸš¨

# Hidden state:
h_9[2] = 0.97 * tanh(-2.817)
       = 0.97 * (-0.993)
       = -0.963  â† 96.3% ATTACK CONFIDENCE!
```

### 9.5 Complete Hidden State Sequence

```python
# Collect all hidden states:
H = np.array([h_0, h_1, h_2, h_3, h_4, h_5, h_6, h_7, h_8, h_9])

print(f"Hidden state sequence shape: {H.shape}")
# Output: (10, 64)  # 10 time steps Ã— 64 hidden units

# Key hidden unit (index 2) tracking attack:
attack_signal = H[:, 2]
print(f"Attack signal evolution: {attack_signal}")
# Output: [-0.641, -0.850, -0.910, -0.935, -0.948, 
#          -0.957, -0.963, -0.971, -0.978, -0.963]

# Visualization:
import matplotlib.pyplot as plt
plt.plot(range(10), attack_signal, marker='o', color='red')
plt.axhline(y=-0.5, color='orange', linestyle='--', label='Suspicious')
plt.axhline(y=-0.9, color='red', linestyle='--', label='Attack!')
plt.xlabel('Time Step')
plt.ylabel('Attack Signal (h_t[2])')
plt.title('Quantum LSTM: Attack Pattern Recognition')
plt.legend()
plt.grid(True)
# Shows CLEAR EXPONENTIAL GROWTH pattern!
```

### 9.6 Final LSTM Output for Classification

**We use the LAST hidden state h_9:**
```python
final_hidden = h_9  # Shape: (64,)

# This represents the LSTM's final "understanding" of the entire sequence
# It has accumulated 10 time steps of attack evidence!

print(f"Final hidden state (first 8 units): {final_hidden[0:8]}")
# Output: [0.092, -0.963, -0.978, -0.945, -0.821, 0.034, -0.956, 0.018]
#          â†‘       â†‘       â†‘       â†‘       â†‘              â†‘
#         ok    ATTACK! ATTACK! ATTACK! ATTACK!        ATTACK!

# Strong negative values = ATTACK DETECTED ACROSS TIME! ğŸš¨
```

### 9.7 Why Quantum LSTM is Powerful

**Classical LSTM challenges:**
```python
- 77 features Ã— 10 steps = 770 values to process
- Vanishing gradients over long sequences
- Difficulty capturing subtle correlations
```

**Quantum LSTM advantages:**
```python
âœ“ Quantum superposition: Process multiple patterns simultaneously
âœ“ Entanglement: Capture complex feature correlations naturally
âœ“ Interference: Amplify attack patterns, suppress noise
âœ“ Memory: Cell state accumulates evidence across time

Result: Better temporal pattern recognition! ğŸ¯
```

### 9.8 Code: Complete Quantum LSTM Forward Pass

```python
class QuantumLSTM(nn.Module):
    def __init__(self, input_size=80, hidden_size=64, n_qubits=4):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.n_qubits = n_qubits
        self.n_groups = hidden_size // n_qubits  # 16 groups
        
        # Learnable quantum parameters for each gate
        self.forget_params = nn.Parameter(torch.randn(self.n_groups, 2, n_qubits))
        self.input_params = nn.Parameter(torch.randn(self.n_groups, 2, n_qubits))
        self.candidate_params = nn.Parameter(torch.randn(self.n_groups, 2, n_qubits))
        self.output_params = nn.Parameter(torch.randn(self.n_groups, 2, n_qubits))
        
    def forward(self, x):
        """
        Args:
            x: (batch, time, input_size) quantum features from CNN
        
        Returns:
            h_final: (batch, hidden_size) final hidden state
        """
        batch, time, _ = x.shape
        
        # Initialize states
        h_t = torch.zeros(batch, self.hidden_size)
        c_t = torch.zeros(batch, self.hidden_size)
        
        # Process each time step
        for t in range(time):
            x_t = x[:, t, :]  # (batch, input_size)
            
            # Compute gates (simplified - actual uses quantum circuits)
            f_t = self.quantum_forget(x_t, h_t)
            i_t = self.quantum_input(x_t, h_t)
            g_t = self.quantum_candidate(x_t, h_t)
            o_t = self.quantum_output(x_t, h_t)
            
            # Update cell state
            c_t = f_t * c_t + i_t * g_t
            
            # Update hidden state
            h_t = o_t * torch.tanh(c_t)
        
        # Return final hidden state
        return h_t  # (batch, 64)

# Usage:
qlstm = QuantumLSTM(input_size=80, hidden_size=64)
final_hidden = qlstm(X_quantum)  # X_quantum: (1, 10, 80)

print(f"Final hidden state shape: {final_hidden.shape}")  # (1, 64)
```

### 9.9 Summary: Quantum LSTM Complete

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      QUANTUM LSTM TEMPORAL PROCESSING COMPLETE       â”‚
â”‚                                                      â”‚
â”‚  Input:  (10, 80) quantum feature sequence           â”‚
â”‚                                                      â”‚
â”‚  Process: 10 time steps, each with:                  â”‚
â”‚    - Forget gate: Keep relevant attack patterns     â”‚
â”‚    - Input gate: Store new attack evidence          â”‚
â”‚    - Candidate: Compute attack indicators           â”‚
â”‚    - Output gate: Expose accumulated evidence       â”‚
â”‚                                                      â”‚
â”‚  Temporal Evolution:                                 â”‚
â”‚    t=0: h[2] = -0.641  â† Attack starts               â”‚
â”‚    t=5: h[2] = -0.957  â† Growing confidence          â”‚
â”‚    t=9: h[2] = -0.963  â† 96% certain! ğŸš¨             â”‚
â”‚                                                      â”‚
â”‚  Output: h_9 = (64,) final hidden state              â”‚
â”‚          Strong negative values = ATTACK!            â”‚
â”‚                                                      â”‚
â”‚  âœ“ Ready for dense classification layers!           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**STATUS: Sections 7-9 Complete (Quantum LSTM)!**  
Next: Dense Layers & Classification (Part 4) - Converting quantum features to final prediction!

